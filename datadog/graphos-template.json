{
  "title": "GraphOS Runtime Dashboard Template",
  "description": "[[suggested_dashboards]]",
  "widgets": [
    {
      "id": 8946993247458357,
      "definition": {
        "title": "Request Traffic & Health: Client → Router",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 4766111693665422,
            "definition": {
              "type": "note",
              "content": "**Recommended SLOs & Alerts:**\n\n-   Alert on sustained increases in 5xx responses from router\n-   If your organization has control over all clients of your supergraph:\n    -   alert on increases in 4xx responses indicating a potential misconfiguration of one or more clients.\n    -   alert on increases in GraphQL errors indicating bad queries from one or more clients.\n-   Reliability SLO: ≥99.9% of Router responses should be 2xx and without error.\n\n### Throughput Categories\n\n**Low Throughput:** Fewer than **10 requests per minute (RPM)** (~0.17 RPS)\n-  Metrics like P95 latency and error rates may be unreliable. You’re likely looking at isolated client activity or background tasks due to the api not being warm.\n-  Monitor with logs, traces, or uptime checks, not SLO-based alerts\n\n**Moderate Throughput:** Between **10 and 100 RPM** (~0.17 to ~1.7 RPS)\n-   Enough volume to detect trends, but not enough to be confident in moment-to-moment alerting.\n-  Use conservative thresholds on any alerts and look for trends over longer windows of time\n\n**High _(enough)_ Throughput:** Greater than **100 RPM** (>1.7 RPS)\n-  Generally enough for real-time alerting and solid SLO enforcement",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 6
            }
          },
          {
            "id": 1224218507683046,
            "definition": {
              "title": "Volume of Requests Per Status Code",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:http.server.request.duration{$service,$env,$version} by {http.response.status_code}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "bars"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear"
              },
              "custom_links": []
            },
            "layout": {
              "x": 0,
              "y": 6,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 5890661141790956,
            "definition": {
              "title": "Throughput: Requests Per Second",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:http.server.request.duration{$service,$env,$version}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "Requests per second",
                      "formula": "anomalies(throughput(query1), 'basic', 5)"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ]
            },
            "layout": {
              "x": 6,
              "y": 6,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 8270570877484838,
            "definition": {
              "type": "note",
              "content": "This chart shows the volume of requests Apollo Router is handling over time, broken down by status code.\n\n**What to look for**\n\n- Are most requests successful (`2xx`)?\n- Are clients misbehaving (`4xx`)?\n- Is the router or a subgraph failing (`5xx`)?\n\n**Why it matters**\n\n- An increase in  `4xx` errors may indicate client bugs or attempts to invoke deprecated queries.\n- A spike in `5xx` errors likely indicates that something is broken downstream.\n- Changes in total volume may reflect traffic surges or outages.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 10,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 7248723417019108,
            "definition": {
              "type": "note",
              "content": "This chart shows requests per second (RPS) hitting the Apollo Router. This is your clearest view into how much work the Router is doing right now.\n\n**What to look for**\n\n- A sudden drop may indicate client outages or configuration errors.\n- A flat line during known busy periods may indicate that you are hitting throttling limits.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 10,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 5348236615021926,
            "definition": {
              "title": "Error Rate Percent",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "time": {},
              "type": "timeseries",
              "requests": [
                {
                  "formulas": [
                    {
                      "alias": "Error Rate (%)",
                      "formula": "anomalies(((query1 + query2) / query3) * 100, 'basic', 2)"
                    }
                  ],
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "count:http.server.request.duration{$service,$env,$version, graphql.errors:true}.as_count()"
                    },
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "count:http.server.request.duration{$service,$env,$version,!http.response.status_code:2*,!http.response.status_code:4*}.as_count()"
                    },
                    {
                      "name": "query3",
                      "data_source": "metrics",
                      "query": "count:http.server.request.duration{$service,$env,$version}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "tags",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ]
            },
            "layout": {
              "x": 0,
              "y": 12,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 6132914662720110,
            "definition": {
              "type": "note",
              "content": "This chart displays the total error rate percentage of requests handled by the Router. It includes:\n\n-   **GraphQL errors** (e.g., validation failures, resolver exceptions).\n-   **HTTP 5xx responses**, typically indicating internal or subgraph infrastructure issues.\n\nThe formula adds these two failure categories together and divides by the total request count, giving a single view into how often requests fail from the client’s perspective.\n\n***\n\n### Why It Matters\n\nUnderstanding total error rate is critical for:\n\n-   **Reliability tracking**: Errors reflect user-facing failures and service health.\n-   **Incident detection**: Elevated error rates can signal broken schemas, subgraph failures, or deploy issues.\n-   **SLO compliance**: Helps teams track whether reliability objectives (like <1% error rate) are being met.\n\n***\n\n### What to Watch Out For\n\n-   **Spikes in error rate**, especially following deploys or traffic increases.\n-   **Sustained error rates above 1–2%**, which may indicate regressions or uncaught edge cases.\n-   **Increased 5xx errors** without corresponding GraphQL errors—this may point to subgraph crashes or timeouts.\n-   **Silent drops in traffic**: If error rate decreases while request volume drops, it could signal issues upstream (e.g., client timeouts or DNS problems).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 16,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 0,
        "width": 12,
        "height": 19
      }
    },
    {
      "id": 3840969086320674,
      "definition": {
        "title": " Request Characteristics: Client → Router",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 989162476390562,
            "definition": {
              "title": "Request Body Size (p99 / Max)",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "vertical",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "max:http.server.request.body.size{$service,$env,$version}"
                    },
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "p99:http.server.request.body.size{$service,$env,$version}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "max",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "byte_in_binary_bytes_family"
                        }
                      },
                      "formula": "query1"
                    },
                    {
                      "alias": "p99",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "byte_in_binary_bytes_family"
                        }
                      },
                      "formula": "query2"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear"
              },
              "custom_links": []
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 4330685396345820,
            "definition": {
              "type": "note",
              "content": "This chart tracks the size of incoming HTTP request bodies over time, highlighting the largest payloads seen.\n\n### Why it matters\n\n-   **`p99`** shows typical large requests.\n-   **`max`** shows the absolute biggest — helpful for spotting outliers.\n\nLarge payloads can:\n\n-   Increase parsing/validation time.\n-   Cause memory pressure or latency spikes.\n-   Indicate misuse (e.g., file uploads, bloated mutations).\n\n### What to look for\n\n-   A stable `p99` suggests consistent request sizes.\n-   Occasional `max` spikes are okay — frequent ones may require investigation.\n-   Rising trends could signal a client change or growing inefficiency.\n\n### Caveats\n\n-   Doesn’t reflect query complexity — a small body can still be expensive.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 4,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 19,
        "width": 12,
        "height": 7
      }
    },
    {
      "id": 2547398354023131,
      "definition": {
        "title": "Request Performance & Latency: Client → Router",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 4277427124049773,
            "definition": {
              "type": "note",
              "content": "**Apollo Router Latency Overview**\n\nApollo Router routes incoming GraphQL requests to subgraphs, handles query planning, and coordinates responses. Latency here reflects the total time the Router takes to process and respond to an HTTP request, including:\n\n-   Parsing and validating the GraphQL query\n-   Calling subgraphs\n-   Merging responses, handling errors, and formatting results\n-   Any middleware or plugin logic you've added\n\nIf your latency rises, the cause could be the Router itself, one or more subgraphs, or custom logic you've installed.",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 3
            }
          },
          {
            "id": 5087230123283744,
            "definition": {
              "title": "Request Duration Distribution",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "type": "distribution",
              "xaxis": {
                "scale": "linear",
                "min": "auto",
                "max": "auto",
                "include_zero": true
              },
              "yaxis": {
                "scale": "linear",
                "min": "auto",
                "max": "auto",
                "include_zero": true
              },
              "requests": [
                {
                  "request_type": "histogram",
                  "query": {
                    "data_source": "metrics",
                    "name": "query1",
                    "aggregator": "avg",
                    "query": "avg:http.server.request.duration{$service,$env,$version}"
                  }
                }
              ]
            },
            "layout": {
              "x": 0,
              "y": 3,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 73435209590847,
            "definition": {
              "title": "Request Duration Percentiles",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "value"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "formulas": [
                    {
                      "alias": "p99",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "formula": "query2"
                    },
                    {
                      "alias": "p95",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "formula": "query3"
                    },
                    {
                      "alias": "p90",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "formula": "query4"
                    },
                    {
                      "alias": "Min",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "formula": "query5"
                    },
                    {
                      "alias": "Max",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "formula": "query6"
                    }
                  ],
                  "queries": [
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "p99:http.server.request.duration{$service,$env,$version}"
                    },
                    {
                      "name": "query3",
                      "data_source": "metrics",
                      "query": "p95:http.server.request.duration{$service,$env,$version}"
                    },
                    {
                      "name": "query4",
                      "data_source": "metrics",
                      "query": "p90:http.server.request.duration{$service,$env,$version}"
                    },
                    {
                      "name": "query5",
                      "data_source": "metrics",
                      "query": "min:http.server.request.duration{$service,$env,$version}"
                    },
                    {
                      "name": "query6",
                      "data_source": "metrics",
                      "query": "max:http.server.request.duration{$service,$env,$version}"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ],
              "yaxis": {
                "include_zero": false,
                "scale": "log"
              },
              "custom_links": []
            },
            "layout": {
              "x": 6,
              "y": 3,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 6514407841345693,
            "definition": {
              "type": "note",
              "content": "This chart shows the distribution of HTTP request durations handled by the Apollo Router.\n\n-   **Metric:** [http.server.request.duration](https://opentelemetry.io/docs/specs/semconv/http/http-metrics/#metric-httpserverrequestduration) — measures how long each HTTP request takes.\n\n### What this tells you\n\n-   The shape of the distribution helps identify performance patterns:\n\n    -   A narrow peak near the left means most requests are fast.\n    -   A long tail to the right indicates some requests are slower and may require investigation.\n\n-   Sudden shifts or broadening of the distribution can indicate latency issues or backend slowdowns.\n\n### Why it matters\n\nMonitoring request duration distribution helps you:\n\n-   Understand if performance issues impact many requests or just a few outliers.\n-   Make informed decisions on where to focus optimization or debugging efforts.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 7,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 2631813081995925,
            "definition": {
              "type": "note",
              "content": "### What the percentiles tell you\n\n-   **p90 / p95:** These percentiles represent the upper range of _typical_ request latencies.\\\n    If these values rise, it generally means a significant portion of requests are experiencing slower responses. Possible causes include:\n\n    -   A subgraph is responding more slowly than usual (e.g., due to load or inefficiency)\n    -   The Router’s query planning step is taking longer (especially with complex or highly federated queries)\n    -   Overall increased load causing processing delays\n\n-   **p99:** This is a _tail latency_ metric that reflects the slowest 1% of requests.\\\n    Spikes here often indicate rare but impactful delays, such as:\n\n    -   Intermittent subgraph performance issues or temporary timeouts\n    -   Specific heavy or complex queries that take much longer to plan or execute\n    -   Unusual overhead in middleware or Router internals affecting some requests\n\n-   **Max:** The absolute slowest observed request during the timeframe.\\\n    Isolated high max values are usually less concerning (e.g., caused by a single unusual request). However, frequent or sustained high max latencies may point to:\n\n    -   Clients issuing extremely large or repeated queries causing resource exhaustion or retries\n    -   Transient infrastructure problems (network issues, DNS delays, resource contention)\n\n-   **Min:** Represents the fastest possible response from the Router. A surprisingly high minimum usually reflects baseline overhead within the Router itself — such as:\n\n    -   Middleware or plugin execution time\n    \n    -   Query parsing, validation, or planning\n    \n    -   Routing logic that's inherently non-negligible\n\n### Caveats\n\n-   Low request volumes can skew percentiles; a single slow request distorts p99 or max.\n-   Slow subgraphs inflate Router latency here—this chart shows total request time, not internal breakdowns.\n-   Latency spikes without load increase may point to network hiccups, subgraph cold starts, or cloud platform noise outside the Router’s control.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 7,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 26,
        "width": 12,
        "height": 10
      }
    },
    {
      "id": 2103055858802167,
      "definition": {
        "title": "Request Traffic & Health: Router → Subgraph",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 8928058460124150,
            "definition": {
              "type": "note",
              "content": "**Recommended SLOs & Alerts:**\n\n-   Alert on sustained increases in 5xx responses from any subgraph.\n-   Reliability SLO: ≥99.9% of Router→subgraph requests should be 2xx and without error.\n\n### Throughput Categories\n\n**Low Throughput:** Fewer than **10 requests per minute (RPM)** (~0.17 RPS)\n-  Metrics like P95 latency and error rates may be unreliable. You’re likely looking at isolated client activity or background tasks due to the api not being warm.\n-  Monitor with logs, traces, or uptime checks, not SLO-based alerts\n\n**Moderate Throughput:** Between **10 and 100 RPM** (~0.17 to ~1.7 RPS)\n-   Enough volume to detect trends, but not enough to be confident in moment-to-moment alerting.\n-  Use conservative thresholds on any alerts and look for trends over longer windows of time\n\n**High _(enough)_ Throughput:** Greater than **100 RPM** (>1.7 RPS)\n-  Generally enough for real-time alerting and solid SLO enforcement",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 5
            }
          },
          {
            "id": 3931875108914008,
            "definition": {
              "title": "Http Requests by Status Code",
              "title_size": "16",
              "title_align": "left",
              "show_legend": false,
              "legend_layout": "vertical",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "formulas": [
                    {
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:http.client.request.duration{$service, $env ,$version} by {subgraph.name,http.response.status_code}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "bars"
                }
              ]
            },
            "layout": {
              "x": 0,
              "y": 5,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 8436708637768666,
            "definition": {
              "title": "Throughput (Requests per Second)",
              "title_size": "16",
              "title_align": "left",
              "show_legend": false,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:http.client.request.duration{$service, $env ,$version} by {subgraph.name}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "Requests per second",
                      "formula": "anomalies(throughput(query1), 'basic', 4)"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ]
            },
            "layout": {
              "x": 6,
              "y": 5,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 1604866605637428,
            "definition": {
              "type": "note",
              "content": "This chart shows the volume of outgoing HTTP requests from the Router to subgraphs, broken down by HTTP status code and subgraph.\n\n**Why it matters:**\n\n-   Tracks overall subgraph traffic.\n\nWhat to look for:\n\n-   High volume of `2xx` is normal and healthy.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 9,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 8148807184808558,
            "definition": {
              "type": "note",
              "content": "This chart shows how many requests per second (RPS) each subgraph is handling. It's useful for understanding traffic distribution across your federated architecture and identifying which subgraphs are the busiest.\n\n**What to Look For:**\n\n-   **Consistently high RPS** on a subgraph may indicate:\n\n    -   It supports popular or frequently queried fields.\n    -   It could become a performance bottleneck if not properly scaled or optimized.\n\n-   **Sudden spikes** can signal usage surges or potential issues downstream (e.g., retry storms).\n\n-   **Low RPS subgraphs** might be underutilized or intentionally lightweight.\n\n**Actionable Insights:**\n\n-   Ensure high-throughput subgraphs are well-instrumented and autoscaled where possible.\n-   Investigate traffic spikes for correlation with client activity or backend issues.\n-   Cross-reference with latency charts to catch early signs of degradation under load.\n\n**Caveats:**\n\n-   This reflects traffic from the Router to each subgraph, not necessarily user-facing load.\n-   Subgraphs handling many small, fast requests may show high RPS but minimal latency or load — context is key.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 9,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 3375905945773031,
            "definition": {
              "title": "Non-2xx Responses",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "vertical",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "formulas": [
                    {
                      "alias": "Sum",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "query": "count:http.client.request.duration{subgraph.name:*,!http.response.status_code:2* ,$service, $env ,$version} by {subgraph.name,http.response.status_code}.as_count()",
                      "data_source": "metrics",
                      "name": "query1"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "dotted",
                    "line_width": "normal"
                  },
                  "display_type": "bars"
                }
              ],
              "yaxis": {
                "include_zero": false
              },
              "markers": []
            },
            "layout": {
              "x": 0,
              "y": 11,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 2117758270276748,
            "definition": {
              "title": "GraphQL Errors by Operation",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "spans",
                      "search": {
                        "query": "status:error $service $env $version"
                      },
                      "indexes": [
                        "*"
                      ],
                      "group_by": [
                        {
                          "facet": "@graphql.operation.name",
                          "limit": 1000,
                          "sort": {
                            "aggregation": "count",
                            "order": "desc",
                            "metric": "count"
                          },
                          "should_exclude_missing": true
                        }
                      ],
                      "compute": {
                        "aggregation": "count"
                      },
                      "storage": "hot"
                    }
                  ],
                  "formulas": [
                    {
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "bars"
                }
              ]
            },
            "layout": {
              "x": 6,
              "y": 11,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 6943674061818587,
            "definition": {
              "type": "note",
              "content": "This chart focuses specifically on non-successful(non-2xx) responses from subgraphs — grouped by subgraph and status code.\n\n**Why it matters:**\n\n-   These are the requests that failed, either due to config errors (4xx) or server-side issues (5xx).\n-   The Router isn’t failing — it’s forwarding a query that the subgraph can't fulfill.\n\n**Common causes of 4xx:**\n\n-   Schema mismatch between subgraph and Supergraph (e.g., schema drift)\n-   Missing auth headers or invalid inputs\n\n**Common causes of 5xx:**\n\n-   Crashes, timeouts, or errors in a subgraphs logic\n\n**How this impacts Router performance:**\n\n-   Frequent 5xxs inflate latency as the Router retries or collects error details.\n-   Repeated 4xxs may be harmless, but still consume Router resources and can signal client or schema issues.\n\n\n**Pro tip:**\\\nIf a subgraph frequently appears here, coordinate with its owning team to investigate error logs or validate schema compatibility.\n\n**Caveats**\n- You might see `N/A` as a status code. This means that a request was made but a response was not received. ",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 15,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 7728601524817940,
            "definition": {
              "type": "note",
              "content": "This chart focuses specifically on GraphQL errors from subgraphs — grouped by query operation name.\n\n**Why it matters:**\n\n-   These are the requests that reported back a 2xx response, but with errors within the actual GraphQL response.\n-   These do not directly impact router performance, but can be an indicator of incorrect logic on your client or subgraph side.\n\n**Caveats**\n\n- This chart is populated via traces, not metrics, so the numbers will not necessarily be absolute",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 15,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 36,
        "width": 12,
        "height": 18
      }
    },
    {
      "id": 7372185579967424,
      "definition": {
        "title": " Request Characteristics: Router → Subgraph",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 5703355397898961,
            "definition": {
              "title": "Response Body Size",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "formulas": [
                    {
                      "alias": "Sum",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "byte_in_decimal_bytes_family"
                        }
                      },
                      "formula": "query4"
                    }
                  ],
                  "queries": [
                    {
                      "name": "query4",
                      "data_source": "metrics",
                      "query": "avg:http.client.response.body.size{$service, $env ,$version} by {subgraph.name}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ],
              "markers": []
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 5212470527428142,
            "definition": {
              "type": "note",
              "content": "This chart tracks the total volume of response data returned by each subgraph over time. It helps identify subgraphs that are consistently returning large payloads, which can impact overall performance and client-side load times.\n\n**What to Look For:**\n\n-   **Spikes or sustained high values** from a subgraph may indicate:\n\n    -   Overfetching: unnecessary fields being resolved and returned.\n    -   Inefficient schema design: returning deeply nested or verbose structures.\n    -   Missing pagination or filtering on list-type fields.\n\n-   **Flat, low-volume subgraphs** could either be healthy or underutilized — context matters.\n\n**Actionable Insights:**\n\n-   Audit GraphQL queries hitting high-volume subgraphs.\n-   Consider schema redesigns to reduce payload size (e.g., use fragments, `@defer`, or pagination).\n-   If the size is expected (e.g., media, large objects), ensure proper compression is in place.\n\n**Caveats:**\n\n-   Values are based on the `Content-Length` header and may not reflect actual decompressed payload size.\n-   This metric tracks data size **returned by subgraphs**, not what is ultimately sent to the client by the router.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 4,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 54,
        "width": 12,
        "height": 7
      }
    },
    {
      "id": 1973549063927411,
      "definition": {
        "title": "Request Performance & Latency: Router → Subgraph",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 2538570288917588,
            "definition": {
              "type": "note",
              "content": "**Recommended SLOs & Alerts:**\n\n-   Alert on P95 latency exceeding thresholds for critical subgraphs.\n-   Latency SLO (For High-Throughput): P95 Router → Subgraph latency should remain under 300ms\n-   Latency SLO (For Moderate-Throughput): Set threshold to (current P95 for the last week + 20%). This allows for natural variability while detecting regressions.\n",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 2
            }
          },
          {
            "id": 7559571845461086,
            "definition": {
              "title": "P95 Latency",
              "title_size": "16",
              "title_align": "left",
              "show_legend": false,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "p95:http.client.request.duration{$service, $env ,$version} by {subgraph.name}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "P95 Latency (Seconds)",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ]
            },
            "layout": {
              "x": 0,
              "y": 2,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 5190503484238670,
            "definition": {
              "title": "Latency by GraphQL Operation",
              "title_size": "16",
              "title_align": "left",
              "type": "query_table",
              "requests": [
                {
                  "queries": [
                    {
                      "name": "query2",
                      "data_source": "spans",
                      "search": {
                        "query": "$service $env $version"
                      },
                      "indexes": [
                        "*"
                      ],
                      "group_by": [
                        {
                          "facet": "@graphql.operation.name",
                          "limit": 1000,
                          "sort": {
                            "aggregation": "pc95",
                            "order": "desc",
                            "metric": "@duration"
                          },
                          "should_exclude_missing": true
                        }
                      ],
                      "compute": {
                        "aggregation": "pc95",
                        "metric": "@duration"
                      },
                      "storage": "hot"
                    }
                  ],
                  "response_format": "scalar",
                  "sort": {
                    "count": 1000,
                    "order_by": [
                      {
                        "type": "formula",
                        "index": 0,
                        "order": "desc"
                      }
                    ]
                  },
                  "formulas": [
                    {
                      "cell_display_mode": "bar",
                      "alias": "p95 duration",
                      "formula": "query2"
                    }
                  ]
                }
              ],
              "has_search_bar": "auto"
            },
            "layout": {
              "x": 6,
              "y": 2,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 422220421274293,
            "definition": {
              "type": "note",
              "content": "This chart tracks the 95th percentile (P95) request duration from the Router to each subgraph.\n\n### Why It Matters\n\n-   P95 is more **resilient to outliers** than max but still reveals tail latency problems that affect real users.\n\n### What to Look For\n\n-  **Rising P95** values can indicate:\n\n    -   Backend slowness\n    -   Schema changes causing more complex queries\n    -   Increased load on subgraphs\n\n-  **Flat or low P95** suggests stable performance under current conditions.\n\n-  **Correlate with throughput** — rising latency during peak traffic may mean you’re under-provisioned.\n\n\n### Actionable Insights\n\n-   Investigate consistently high-latency subgraphs — even if they aren’t crashing, they may be degrading user experience.\n-   Compare with response body size and throughput to find potential root causes (e.g., payload bloat or traffic spikes).\n-   Check if retry or @requires patterns are contributing.\n-   Consider the use of the [@defer](https://www.apollographql.com/docs/graphos/routing/operations/defer) directive to handle fields that take a particularly long time to resolve.\n\n\n### Caveats\n\n-   P95 latency is unreliable for low-throughput subgraphs because small sample sizes let outliers disproportionately skew the metric, making it hard to distinguish real issues from statistical noise.\n-   P95 doesn’t reveal all latency issues — check the distribution chart for outliers and long tails.\n",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 6,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 8992800975500272,
            "definition": {
              "type": "note",
              "content": "This chart tracks the p95 request duration from the Router to each GraphQL operation for your top 1000 operations.\n\nWhy it matters\n\n-   Captures the end-to-end time it takes the Router to process and respond to a specific operation.\n-   Helps identify slow or expensive operations affecting user experience.\n\nWhat to look for\n\n-   Operations should have consistent, predictable durations over time.\n-   Rising p95s may indicate schema changes, degraded subgraph performance, or increased data complexity.\n-   Watch for high-variance or long-tail operations that may benefit from optimization or caching.\n\n### Caveats\n\n-   These values are retrieved via sampling to avoid the cost of high-cardinality metrics ingestion, so they are not 100% accurate especially over small time windows",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 6,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 4614292295630845,
            "definition": {
              "title": "Subgraph Performance Profile",
              "title_size": "16",
              "title_align": "left",
              "type": "scatterplot",
              "requests": {
                "table": {
                  "response_format": "scalar",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:http.client.request.duration{$service, $env ,$version} by {subgraph.name}.as_count()",
                      "aggregator": "sum"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query2",
                      "query": "p95:http.client.request.duration{$service, $env ,$version} by {subgraph.name}",
                      "aggregator": "max"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "Requests",
                      "dimension": "x",
                      "formula": "throughput(query1)"
                    },
                    {
                      "dimension": "y",
                      "alias": "P95 Latency (Seconds)",
                      "formula": "query2"
                    }
                  ]
                }
              },
              "xaxis": {
                "scale": "linear",
                "include_zero": true,
                "min": "auto",
                "max": "auto"
              },
              "yaxis": {
                "scale": "linear",
                "include_zero": true,
                "min": "auto",
                "max": "auto"
              },
              "color_by_groups": [
                "subgraph.name"
              ]
            },
            "layout": {
              "x": 0,
              "y": 8,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 6179363342034765,
            "definition": {
              "title": "Subgraph Performance Profile",
              "title_size": "16",
              "title_align": "left",
              "time": {
                "type": "live",
                "unit": "week",
                "value": 1
              },
              "type": "scatterplot",
              "requests": {
                "table": {
                  "response_format": "scalar",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:http.client.request.duration{$service, $env ,$version} by {subgraph.name}.as_count()",
                      "aggregator": "sum"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query2",
                      "query": "p95:http.client.request.duration{$service, $env ,$version} by {subgraph.name}",
                      "aggregator": "max"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "Requests",
                      "dimension": "x",
                      "formula": "throughput(query1)"
                    },
                    {
                      "dimension": "y",
                      "alias": "P95 Latency (Seconds)",
                      "formula": "query2"
                    }
                  ]
                }
              },
              "xaxis": {
                "scale": "linear",
                "include_zero": true,
                "min": "auto",
                "max": "auto"
              },
              "yaxis": {
                "scale": "linear",
                "include_zero": true,
                "min": "auto",
                "max": "auto"
              },
              "color_by_groups": [
                "subgraph.name"
              ]
            },
            "layout": {
              "x": 6,
              "y": 8,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 6615090037432379,
            "definition": {
              "type": "note",
              "content": "This chart aids in assessing the performance of individual subgraphs by correlating their request handling with their response times in order to identify hotspots and bottlenecks. \n\n**What to Look For:**\n- **Top right** = _hotspot / bottleneck_\n- **Bottom left** = _healthy_\n\nGood\n-   **Bottom-Right Quadrant (High RPS & Low Latency):** Ideal scenario indicating efficient subgraphs.\n-   **Bottom-Left Quadrant (Low RPS & Low Latency):** Efficient but underutilized.\n\nWarning\n-   **Top-Left Quadrant (Low RPS & High Latency):** Underutilized and slow; potential future bottlenecks or issues if traffic increases. \n\nBad\n-   **Top-Right Quadrant (High RPS & High Latency ):** Subgraphs handling many requests with slower responses; may need optimization.\n\n**Actionable Insights:**\n\n-   **Optimize High-Latency Subgraphs:** Perform schema optimizations or improve the performance of subgraphs. Starting with ones in the top right. \n-   **Annotate Known Outliers:** Use a note widget or annotations to explain any known anomalies:\n    -  “This subgraph is intentionally slow due to external API call.”\n    -  “This one was recently scaled — latency is expected to improve.”\n\n**Caveats:**\n\n- **Axes Auto-Scale:** This scatterplot dynamically adjusts its axes based on the current data. As a result, a subgraph may appear in the top-right corner simply due to a narrow overall range — not necessarily because it has poor performance. Always interpret positions relative to other subgraphs, and remember that a top-right placement can still be acceptable if the latency and throughput are within expected bounds",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 12,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 1856669505570071,
            "definition": {
              "type": "note",
              "content": "This chart is a companion to the scatter plot to the left, set to a fixed time window of 1 week to provide a reference point against whatever timescale you have your dashboard configured to.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 12,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 9681688645598,
            "definition": {
              "title": "Request Duration Distribution",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "type": "distribution",
              "xaxis": {
                "scale": "linear",
                "min": "auto",
                "max": "auto",
                "include_zero": true
              },
              "yaxis": {
                "scale": "linear",
                "min": "auto",
                "max": "auto",
                "include_zero": true
              },
              "requests": [
                {
                  "request_type": "histogram",
                  "query": {
                    "data_source": "metrics",
                    "name": "query1",
                    "aggregator": "avg",
                    "query": "avg:http.client.request.duration{subgraph.name:* ,$service, $env ,$version}"
                  }
                }
              ]
            },
            "layout": {
              "x": 0,
              "y": 14,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 1275499059031764,
            "definition": {
              "type": "note",
              "content": "This chart visualizes the distribution of subgraph request durations over time. It helps identify how long most requests take and highlights the presence of any outliers or tail latency issues.\n\n### What to Look For\n\n-   A tight cluster around low durations suggests fast, consistent subgraph responses.\n-   A long tail or spread to the right may indicate occasional slowness or outliers.\n-   Sudden shape changes over time can highlight new regressions, changes in traffic, or schema changes.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 18,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 61,
        "width": 12,
        "height": 21
      }
    },
    {
      "id": 5499564723801628,
      "definition": {
        "title": "Top Most Queried Subgraphs: Request Duration Distributions",
        "title_size": "16",
        "title_align": "left",
        "type": "split_group",
        "source_widget_definition": {
          "title": "",
          "title_size": "16",
          "title_align": "left",
          "type": "distribution",
          "xaxis": {
            "scale": "linear",
            "min": "auto",
            "max": "auto",
            "include_zero": true
          },
          "yaxis": {
            "scale": "linear",
            "min": "auto",
            "max": "auto",
            "include_zero": true
          },
          "requests": [
            {
              "request_type": "histogram",
              "query": {
                "data_source": "metrics",
                "name": "query1",
                "aggregator": "avg",
                "query": "avg:http.client.request.duration{subgraph.name:* ,$service, $env ,$version}"
              }
            }
          ]
        },
        "split_config": {
          "split_dimensions": [
            {
              "one_graph_per": "subgraph.name"
            }
          ],
          "limit": 12,
          "sort": {
            "order": "desc",
            "compute": {
              "aggregation": "count",
              "metric": "http.client.request.duration"
            }
          }
        },
        "size": "xs",
        "has_uniform_y_axes": true
      },
      "layout": {
        "x": 0,
        "y": 82,
        "width": 12,
        "height": 5
      }
    },
    {
      "id": 1355103844861603,
      "definition": {
        "title": "Query Planning",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 942676723835878,
            "definition": {
              "type": "note",
              "content": "The query planner is the heart of the runtime's computational efforts. This is where a request to your supergraph is translated into an invocation plan for the appropriate underlying subgraphs. Its performance will be very closely correlated to your supergraph schema and request characteristics, and will be the other determining factor in your response times beyond the subgraphs themselves.",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 1
            }
          },
          {
            "id": 7301212877184944,
            "definition": {
              "title": "Duration and Wait Time",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query2",
                      "query": "avg:apollo.router.compute_jobs.execution.duration{service:$service.value,env:$env.value,version:$version.value ,job.type:query_planning}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "max:apollo.router.compute_jobs.execution.duration{service:$service.value,env:$env.value,version:$version.value , job.type:query_planning}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query3",
                      "query": "avg:apollo.router.compute_jobs.queue.wait.duration{service:$service.value,env:$env.value,version:$version.value , job.type:query_planning}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query4",
                      "query": "max:apollo.router.compute_jobs.queue.wait.duration{service:$service.value,env:$env.value,version:$version.value , job.type:query_planning}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "alias": "avg execution duration",
                      "formula": "query2"
                    },
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "alias": "max execution duration",
                      "formula": "query1"
                    },
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "alias": "avg wait time",
                      "formula": "query3"
                    },
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        }
                      },
                      "alias": "max wait time",
                      "formula": "query4"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "tags",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "formulas": [
                    {
                      "alias": "schema reload",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.schema.load.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                },
                {
                  "formulas": [
                    {
                      "alias": "cache warmup",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.query_planning.warmup.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "sqrt"
              }
            },
            "layout": {
              "x": 0,
              "y": 1,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 3371314754887370,
            "definition": {
              "title": "Evaluated Plans",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query2",
                      "query": "avg:apollo.router.query_planning.plan.evaluated_plans{$service, $env, $version}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "max:apollo.router.query_planning.plan.evaluated_plans{$service, $env, $version}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "alias": "average evaluated",
                      "formula": "anomalies(query2, 'basic', 2)"
                    },
                    {
                      "number_format": {},
                      "alias": "max evaluated",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "tags",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "formulas": [
                    {
                      "alias": "schema reload",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.schema.load.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                },
                {
                  "formulas": [
                    {
                      "alias": "cache warmup",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.query_planning.warmup.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear"
              }
            },
            "layout": {
              "x": 6,
              "y": 1,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 8368245832144593,
            "definition": {
              "type": "note",
              "content": "This chart tracks the wait and execution durations for query planning jobs over time.\n\n**Why it matters**\n\n-   Query planning is critical to routing — delays here increase end-to-end latency.\n-   Schema or configuration changes can unexpectedly impact planning performance.\n-   High max durations may point to specific queries performing poorly in the planner.\n\n**What to look for**\n\n-   Low, stable execution times indicate efficient planning.\n-   Spikes in max duration could signal complex or inefficient queries.\n-   Rising trends after deployments or schema changes may require investigation.\n\n\nFor deeper analysis of poorly performing queries, review [query planning best practices](https://www.apollographql.com/docs/graphos/routing/query-planning/query-planning-best-practices).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 5,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 3057614794403403,
            "definition": {
              "type": "note",
              "content": "This chart shows how many query plans are evaluated before selecting the optimal one for a request.\n\n**Why it matters**\n-   Fewer evaluated plans mean faster query planning and consequently lower CPU usage and response latency.\n\n**What to look for**\n\n-   An average near 1 is ideal — it means the planner is doing the minimal possible work per request.\n-   Occasional spikes in max are expected, but frequent or rising values may indicate overly complex queries.\n-   Sustained increases above 1 could suggest a need to revisit schema design or client queries.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 5,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 87,
        "width": 12,
        "height": 8
      }
    },
    {
      "id": 1015408870961857,
      "definition": {
        "title": "Cache",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 2338005724257191,
            "definition": {
              "type": "note",
              "content": "Whenever your router receives an incoming GraphQL operation, it generates a [query plan](https://www.apollographql.com/docs/federation/query-plans/) to determine which subgraphs it needs to query to resolve that operation. By caching previously generated query plans, your router can _skip_ generating them _again_ if a client later sends the exact same operation. This improves your router's responsiveness.\n\nMonitoring your caching provides insights into both how normal your traffic is and how well your router is able to serve those requests. Your ideal cache performance and hit rates will vary depending on your expected traffic patterns. With the in-memory cache, it will be common to see spikes of cache misses and changes in cache size when new instances are deployed, new schemas are deployed, or new config versions are deployed. During this time, the cache will be evicted and re-warmed up.\n\nThese charts are pre-populated with event overlays for schema reloads and query plan warmup events. Schema reloads will accompany an expected drop in cache record count and a spike in cache miss rate as unusable cache records are evicted. [Cache warmups](https://www.apollographql.com/docs/graphos/routing/performance/caching/in-memory#cache-warm-up) will accompany a period of cache misses as new plans are deliberately hit to prime them into the cache. The router does not serve traffic on the new schema until after that precompute warmup has completed.\n\nFor context on what’s cached and why, review the in-memory caching  [documentation](https://www.apollographql.com/docs/graphos/routing/performance/caching/in-memory) before interpreting the charts below.\n\n**Caveats**\n- These metrics assume that you are running an in-memory cache on your Router. \n\n- If you are using Redis for distributed caching, you will want to monitor that separately [via Datadog's agent](https://docs.datadoghq.com/integrations/redisdb/?tab=host).\n\n- If you are using Entity Caching, [separate metrics will be available](https://www.apollographql.com/docs/graphos/routing/performance/caching/entity#observability) to monitor the performance of that feature.\n\n",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 5
            }
          },
          {
            "id": 280811115102223,
            "definition": {
              "title": "Misses vs. Record Count",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "sum:apollo.router.cache.miss.time.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "cache misses",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "bars"
                },
                {
                  "on_right_yaxis": true,
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "avg:apollo.router.cache.size{service:$service.value,env:$env.value,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "cache record count",
                      "formula": "query0"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "on_right_yaxis": true,
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "sum:apollo.router.cache.hit.time.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.cache.miss.time.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "cache hit %",
                      "formula": "(query0 / (query0 + query1)) * 100"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "formulas": [
                    {
                      "alias": "cache warmup",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.query_planning.warmup.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                },
                {
                  "formulas": [
                    {
                      "alias": "schema reload",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.schema.load.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                }
              ],
              "yaxis": {
                "include_zero": true
              },
              "right_yaxis": {
                "include_zero": false,
                "scale": "log"
              }
            },
            "layout": {
              "x": 0,
              "y": 5,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 5973444432338869,
            "definition": {
              "title": "Record Counts by Instance",
              "title_size": "16",
              "title_align": "left",
              "show_legend": false,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:apollo.router.cache.size{service:$service.value,env:$env.value,version:$version.value} by {host,pod_name,container_id}"
                    }
                  ],
                  "formulas": [
                    {
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "tags",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "formulas": [
                    {
                      "alias": "cache warmup",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.query_planning.warmup.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                },
                {
                  "formulas": [
                    {
                      "alias": "schema reload",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.schema.load.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                }
              ]
            },
            "layout": {
              "x": 6,
              "y": 5,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 3045863848046569,
            "definition": {
              "type": "note",
              "content": "This chart shows an aggregate view across all cache types of your record counts and how many cache misses are happening. This is a lower-cardinality aggregate view of all the other charts in this section, specifically for correlating events between them.\n\n**Why it matters:**\n\n* Your cache sizes impact the memory footprint of your runtime\n* Your cache miss counts represent costlier requests to your runtime\n\n**What to look for:**\n* Drops in record counts that are *not* correlated to schema reloads or new instance deployments\n* Spikes in cache misses that are *not* correlated to a drop in record count\n* Spikes in cache misses that *do* correlate to a significant drop in cache hit percentage",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 9,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 4941765757767393,
            "definition": {
              "type": "note",
              "content": "This chart breaks down the number of records in the cache of each of your runtime instances.\n\n**Why it matters:**\n\n* If your traffic isn't well-distributed across your instances, some caches will lag behind in growth rate\n* If you have an instance go into an unhealthy state, you may see the anomaly represented here\n\n**What to look for:**\n* An instance or instances that have an anomalous count of records without a corresponding deployment or schema change\n* An instance or instances that are growing at a different rate than the others without a corresponding deployment\n\n**Caveat:**\n\nGrouping by `host`, `pod_name`, and `container_id` may cause fragmented data if these tags aren’t consistent or unique in your setup. Update the group-by fields to whatever works best in your environment for accurate results.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 9,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 8120174499044240,
            "definition": {
              "title": "Record Counts by Type",
              "title_size": "16",
              "title_align": "left",
              "show_legend": false,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "sum:apollo.router.cache.size{service:$service.value,env:$env.value,version:$version.value} by {kind,version}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "cache record count",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ],
              "yaxis": {
                "include_zero": false,
                "scale": "log"
              }
            },
            "layout": {
              "x": 0,
              "y": 11,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 8691777953686757,
            "definition": {
              "title": "Misses by Type",
              "title_size": "16",
              "title_align": "left",
              "show_legend": false,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "sum:apollo.router.cache.miss.time.count{service:$service.value,env:$env.value,version:$version.value} by {kind,version}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "cache misses",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "thick"
                  },
                  "display_type": "bars"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear"
              }
            },
            "layout": {
              "x": 6,
              "y": 11,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 7314147141979253,
            "definition": {
              "type": "note",
              "content": "This chart shows cache record counts broken down by each cache type.\n\n**Why it matters:**\n\n* When diagnosing issues with the above aggregate values, it can be informational to see the breakdown at a finer level of detail",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 15,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 5420652058492445,
            "definition": {
              "type": "note",
              "content": "This chart shows cache misses broken down by cache type.\n\n**Why it matters:**\n\n* When diagnosing issues with the above aggregate values, it can be informational to see the breakdown at a finer level of detail",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 15,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 7905054550039923,
            "definition": {
              "title": "Hit % by Instance",
              "title_size": "16",
              "title_align": "left",
              "show_legend": false,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "sum:apollo.router.cache.hit.time.count{service:$service.value,env:$env.value,version:$version.value} by {host,pod_name,container_id}.as_count()"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.cache.miss.time.count{service:$service.value,env:$env.value,version:$version.value} by {host,pod_name,container_id}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "cache hit %",
                      "formula": "(query0 / (query0 + query1)) * 100"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "tags",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "pow",
                "min": "auto",
                "max": "105"
              },
              "markers": []
            },
            "layout": {
              "x": 0,
              "y": 17,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 2128081234153598,
            "definition": {
              "type": "note",
              "content": "This chart shows cache hit rate broken down by each individual running instance of your runtime.\n\n**Why it matters:**\n\n* When diagnosing issues with the above aggregate values, it can be informational to see the breakdown at a finer level of detail\n\n**Caveat:**\n\nGrouping by `host`, `pod_name`, and `container_id` may cause fragmented data if these tags aren’t consistent or unique in your setup. Update the group-by fields to whatever works best in your environment for accurate results.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 21,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 95,
        "width": 12,
        "height": 24
      }
    },
    {
      "id": 3027455303862458,
      "definition": {
        "title": "Compute Jobs",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 7402516847608650,
            "definition": {
              "type": "note",
              "content": "Compute jobs are CPU-intensive tasks that get managed on a dedicated thread pool, inclusive of query planning tasks. Non-query-planning tasks tend to be either low-cost and are consequently less commonly a performance pain point. Most diagnosis will be done against the query planning metrics directly, but this section is provided for comparative purposes.",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 2
            }
          },
          {
            "id": 2646683015980845,
            "definition": {
              "title": "Queued Jobs",
              "title_size": "16",
              "title_align": "left",
              "show_legend": false,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.compute_jobs.queued{service:$service.value,env:$env.value,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "formula": "query1"
                    },
                    {
                      "alias": "queued jobs",
                      "formula": "query1 * 1000"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "bars"
                },
                {
                  "formulas": [
                    {
                      "alias": "schema reload",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.schema.load.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                }
              ]
            },
            "layout": {
              "x": 0,
              "y": 2,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 2242335784104899,
            "definition": {
              "title": "Query Planning Avg Duration and Wait Time",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:apollo.router.compute_jobs.execution.duration{service:$service.value,env:$env.value,version:$version.value, job.type:query_planning}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query3",
                      "query": "avg:apollo.router.compute_jobs.queue.wait.duration{service:$service.value,env:$env.value,version:$version.value, job.type:query_planning}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        },
                        "unit_scale": {
                          "type": "canonical_unit",
                          "unit_name": "millisecond"
                        }
                      },
                      "alias": "avg execution duration",
                      "formula": "query1"
                    },
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        },
                        "unit_scale": {
                          "type": "canonical_unit",
                          "unit_name": "millisecond"
                        }
                      },
                      "alias": "avg wait time",
                      "formula": "query3"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "tags",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "formulas": [
                    {
                      "alias": "schema reload",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.schema.load.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                },
                {
                  "formulas": [
                    {
                      "alias": "cache warmup",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.query_planning.warmup.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear"
              }
            },
            "layout": {
              "x": 6,
              "y": 2,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 5923077997925504,
            "definition": {
              "type": "note",
              "content": "This chart tracks the depth of the compute job queue over time, showing how many jobs are waiting to be processed.\n\n**Why it matters:**\n\n* A queue with a depth greater than 1 means jobs are backing up, introducing latency for any requests that depend on them.\n* Sustained queuing often points to CPU bottlenecks or insufficient compute capacity.\n* Helps identify runtime saturation under load.\n\n**What to look for**\n\n* A queue of 0-1 is ideal — jobs are being processed as they arrive.\n* Short spikes can be normal during brief load surges.\n* Sustained periods above a depth of 1 may require scaling CPU resources or optimizing workload.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 6,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 8220638844765090,
            "definition": {
              "type": "note",
              "content": "This chart tracks the wait and execution durations for query planning jobs over time.\n\n**Why it matters**\n\n-   Query planning is critical to routing — delays here increase end-to-end latency.\n-   Schema or configuration changes can unexpectedly impact planning performance.\n-   High max durations may point to specific queries performing poorly in the planner.\n\n**What to look for**\n\n-   Low, stable execution times indicate efficient planning.\n-   Spikes in max duration could signal complex or inefficient queries.\n-   Rising trends after deployments or schema changes may require investigation.\n\n\nFor deeper analysis of poorly performing queries, review [query planning best practices](https://www.apollographql.com/docs/graphos/routing/query-planning/query-planning-best-practices).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 6,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 6504562094582051,
            "definition": {
              "title": "Query Parsing Avg Duration and Wait Time",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:apollo.router.compute_jobs.execution.duration{service:$service.value,env:$env.value,version:$version.value, job.type:query_parsing}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query3",
                      "query": "avg:apollo.router.compute_jobs.queue.wait.duration{service:$service.value,env:$env.value,version:$version.value ,job.type:query_parsing}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        },
                        "unit_scale": {
                          "type": "canonical_unit",
                          "unit_name": "millisecond"
                        }
                      },
                      "alias": "avg execution duration",
                      "formula": "query1"
                    },
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        },
                        "unit_scale": {
                          "type": "canonical_unit",
                          "unit_name": "millisecond"
                        }
                      },
                      "alias": "avg wait time",
                      "formula": "query3"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "tags",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "formulas": [
                    {
                      "alias": "schema reload",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.schema.load.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                },
                {
                  "formulas": [
                    {
                      "alias": "cache warmup",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.query_planning.warmup.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear"
              }
            },
            "layout": {
              "x": 0,
              "y": 8,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 3496774608895013,
            "definition": {
              "title": "Job Counts by Outcome",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:apollo.router.compute_jobs.duration{service:$service.value,env:$env.value,version:$version.value} by {job.outcome}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "bars"
                },
                {
                  "formulas": [
                    {
                      "alias": "schema reload",
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.schema.load.duration.count{service:$service.value,env:$env.value,version:$version.value}.as_count()"
                    }
                  ],
                  "response_format": "timeseries",
                  "style": {
                    "palette": "gray",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "overlay"
                }
              ]
            },
            "layout": {
              "x": 6,
              "y": 8,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 6257474700798550,
            "definition": {
              "type": "note",
              "content": "This chart tracks how long query parsing jobs take to execute, and how long they wait in the queue before starting.\n\n**Why it matters:**\n\n-   Long execution times can point to expensive or malformed queries.\n-   High wait times indicate the system is queuing work faster than it can process it — a sign of CPU pressure or under-provisioning.\n-   Parsing is part of the critical request path, so delays here directly impact overall latency.\n\n**What to look for**\n\n-   Stable, low execution and wait times suggest healthy parsing performance.\n-   Spikes in execution time may reflect incoming query complexity or anomalies.\n-   Persistent wait time increases likely mean you need to scale compute or investigate query patterns.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 12,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 8647054076741359,
            "definition": {
              "type": "note",
              "content": "This chart shows the total number of compute jobs actively running in your runtime, broken down by success or failure status.\n\n**Why it matters**\n-   Failed jobs may indicate internal issues, misconfigurations, or upstream errors.\n-   Monitoring execution volume helps assess load and throughput.\n\n**What to look for**\n\n-   A steady level of `executed_ok` jobs.\n-   Any presence of failed job types (`executed_error`, `rejected_queue_full`, etc.).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 12,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 119,
        "width": 12,
        "height": 15,
        "is_column_break": true
      }
    },
    {
      "id": 5629804781331548,
      "definition": {
        "title": "Container/Host",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 1060056504123377,
            "definition": {
              "type": "note",
              "content": "Monitoring the resource consumption of your execution environment is essential to ensuring that your runtime can perform as desired.\n\nWe recommend monitoring for and ensuring that your average CPU usage does not exceed 80% load, to leave available overhead for expensive spikes such as schema reloads.\n\nYou can use the [router resource estimator](https://www.apollographql.com/docs/graphos/routing/self-hosted/resource-estimator) to determine what your memory thresholds and allocations should be.\n\nBelow are some example charts of how to plot this data in various deployment environments. They are parameterized using Datadog's [unified service tagging](https://docs.datadoghq.com/getting_started/tagging/unified_service_tagging).",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 2
            }
          },
          {
            "id": 7198505991257678,
            "definition": {
              "title": "Kubernetes CPU Usage",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "avg:kubernetes.cpu.usage.total{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "max:kubernetes.cpu.usage.total{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "alias": "avg CPU",
                      "formula": "query1"
                    },
                    {
                      "number_format": {},
                      "alias": "max CPU",
                      "formula": "query2"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "on_right_yaxis": false,
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "max:kubernetes.cpu.limits{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "min:kubernetes.cpu.limits{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "style": {
                        "palette": "warm",
                        "palette_index": 0
                      },
                      "alias": "max cpu limit",
                      "formula": "query0"
                    },
                    {
                      "number_format": {},
                      "style": {
                        "palette": "warm",
                        "palette_index": 0
                      },
                      "alias": "min cpu limit",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "red",
                    "order_reverse": false,
                    "line_type": "dashed",
                    "line_width": "thin"
                  },
                  "display_type": "line"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear",
                "max": "auto"
              },
              "markers": []
            },
            "layout": {
              "x": 0,
              "y": 2,
              "width": 3,
              "height": 3
            }
          },
          {
            "id": 4808663335885167,
            "definition": {
              "title": "Host CPU Usage (OTEL Collector - Hostmetrics)",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "avg:system.cpu.stolen{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "avg:system.cpu.iowait{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query3",
                      "data_source": "metrics",
                      "query": "avg:system.cpu.system{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query4",
                      "data_source": "metrics",
                      "query": "avg:system.cpu.user{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query5",
                      "data_source": "metrics",
                      "query": "avg:system.cpu.idle{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "style": {
                        "palette": "dog_classic",
                        "palette_index": 4
                      },
                      "alias": "avg stolen",
                      "formula": "query2"
                    },
                    {
                      "number_format": {},
                      "style": {
                        "palette": "dog_classic",
                        "palette_index": 5
                      },
                      "alias": "avg iowait",
                      "formula": "query1"
                    },
                    {
                      "number_format": {},
                      "style": {
                        "palette": "dog_classic",
                        "palette_index": 1
                      },
                      "alias": "avg system",
                      "formula": "query3"
                    },
                    {
                      "number_format": {},
                      "style": {
                        "palette": "dog_classic",
                        "palette_index": 2
                      },
                      "alias": "avg user",
                      "formula": "query4"
                    },
                    {
                      "number_format": {},
                      "style": {
                        "palette": "dog_classic",
                        "palette_index": 0
                      },
                      "alias": "avg idle",
                      "formula": "query5"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "area"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear",
                "min": "0",
                "max": "100"
              },
              "markers": [
                {
                  "label": " Example Threshold ",
                  "value": "y = 75",
                  "display_type": "error dashed"
                }
              ]
            },
            "layout": {
              "x": 3,
              "y": 2,
              "width": 3,
              "height": 3
            }
          },
          {
            "id": 3871424817397328,
            "definition": {
              "title": "Docker CPU Usage (Datadog Docker Agent)",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query3",
                      "data_source": "metrics",
                      "query": "avg:docker.cpu.usage{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "max:docker.cpu.usage{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "style": {
                        "palette": "dog_classic",
                        "palette_index": 1
                      },
                      "alias": "avg cpu",
                      "formula": "query3"
                    },
                    {
                      "number_format": {},
                      "style": {
                        "palette": "dog_classic",
                        "palette_index": 1
                      },
                      "alias": "max cpu",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "area"
                },
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "avg:docker.cpu.limit{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "cpu limit",
                      "formula": "query0"
                    }
                  ],
                  "style": {
                    "palette": "red",
                    "line_type": "dashed",
                    "line_width": "thin"
                  },
                  "display_type": "line"
                }
              ],
              "yaxis": {
                "include_zero": false,
                "scale": "linear",
                "min": "auto",
                "max": "auto"
              },
              "markers": []
            },
            "layout": {
              "x": 6,
              "y": 2,
              "width": 3,
              "height": 3
            }
          },
          {
            "id": 7178237105285863,
            "definition": {
              "title": "Docker CPU Usage (OTEL Collector  - Docker/stats)",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "avg:container.cpu.usage.total{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "max:container.cpu.usage.total{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "alias": "avg cpu",
                      "formula": "query1"
                    },
                    {
                      "number_format": {},
                      "alias": "max cpu",
                      "formula": "query2"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "min:container.cpu.limit{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "max:container.cpu.limit{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "min cpu limit",
                      "style": {
                        "palette": "warm",
                        "palette_index": 0
                      },
                      "formula": "query0"
                    },
                    {
                      "alias": "max cpu limit",
                      "style": {
                        "palette": "warm",
                        "palette_index": 0
                      },
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "red",
                    "color_order": "monotonic",
                    "line_type": "dashed",
                    "line_width": "thin"
                  },
                  "display_type": "line"
                }
              ],
              "yaxis": {
                "include_zero": true,
                "scale": "linear",
                "max": "auto"
              },
              "markers": []
            },
            "layout": {
              "x": 9,
              "y": 2,
              "width": 3,
              "height": 3
            }
          },
          {
            "id": 1123646347411335,
            "definition": {
              "title": "Kubernetes Memory Usage",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "avg:kubernetes.memory.usage{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "max:kubernetes.memory.usage{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "alias": "average memory",
                      "formula": "query1"
                    },
                    {
                      "number_format": {},
                      "alias": "max memory",
                      "formula": "query2"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "min:kubernetes.memory.limits{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "max:kubernetes.memory.limits{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "style": {
                        "palette": "warm",
                        "palette_index": 0
                      },
                      "alias": "min memory limit",
                      "formula": "query0"
                    },
                    {
                      "style": {
                        "palette": "warm",
                        "palette_index": 0
                      },
                      "alias": "max memory limit",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "red",
                    "line_type": "dashed",
                    "line_width": "thin"
                  },
                  "display_type": "line"
                }
              ],
              "markers": []
            },
            "layout": {
              "x": 0,
              "y": 5,
              "width": 3,
              "height": 3
            }
          },
          {
            "id": 7061503711231094,
            "definition": {
              "title": "Host Memory Usage (OTEL Collector - Hostmetrics)",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "avg:system.mem.used{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "max:system.mem.used{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "alias": "average memory",
                      "formula": "query1"
                    },
                    {
                      "number_format": {},
                      "alias": "max memory",
                      "formula": "query2"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "min:system.mem.total{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "max:system.mem.total{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "min available memory",
                      "formula": "query0"
                    },
                    {
                      "alias": "max available memory",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ],
              "markers": [
                {
                  "label": " Example Host Limit ",
                  "value": "y = 150000000000",
                  "display_type": "error dashed"
                }
              ]
            },
            "layout": {
              "x": 3,
              "y": 5,
              "width": 3,
              "height": 3
            }
          },
          {
            "id": 3525357882872541,
            "definition": {
              "title": "Docker Memory Usage (Datadog Docker Agent)",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "avg:docker.mem.in_use{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "max:docker.mem.in_use{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "alias": "average memory",
                      "formula": "query1"
                    },
                    {
                      "number_format": {},
                      "alias": "max memory",
                      "formula": "query2"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "avg:docker.mem.limit{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "memory limit",
                      "formula": "query0"
                    }
                  ],
                  "style": {
                    "palette": "red",
                    "line_type": "dashed",
                    "line_width": "thin"
                  },
                  "display_type": "line"
                }
              ],
              "markers": []
            },
            "layout": {
              "x": 6,
              "y": 5,
              "width": 3,
              "height": 3
            }
          },
          {
            "id": 6913818230766855,
            "definition": {
              "title": "Docker Memory Usage (OTEL Collector  - Docker/stats)",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "name": "query1",
                      "data_source": "metrics",
                      "query": "avg:container.memory.usage.total{service:$service.value ,env:$env.value ,version:$version.value}"
                    },
                    {
                      "name": "query2",
                      "data_source": "metrics",
                      "query": "max:container.memory.usage.total{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "number_format": {},
                      "alias": "average memory",
                      "formula": "query1"
                    },
                    {
                      "number_format": {},
                      "alias": "max memory",
                      "formula": "query2"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "avg:container.memory.usage.limit{service:$service.value ,env:$env.value ,version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "memory limit",
                      "formula": "query0"
                    }
                  ],
                  "style": {
                    "palette": "red",
                    "line_type": "dashed",
                    "line_width": "thin"
                  },
                  "display_type": "line"
                }
              ],
              "markers": []
            },
            "layout": {
              "x": 9,
              "y": 5,
              "width": 3,
              "height": 3
            }
          },
          {
            "id": 7008282361541690,
            "definition": {
              "type": "note",
              "content": "These charts draw their threshold marker dynamically based on the limits specified in Kubernetes for memory and CPU. If your runtime is deployed across heterogeneous container configurations for those values, the dynamic thresholds will display both the min and the max values.\n\nFor more details on available Kubernetes metrics, see the full [documentation on the Datadog Kubernetes collector](https://docs.datadoghq.com/containers/kubernetes/data_collected/).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 8,
              "width": 3,
              "height": 2
            }
          },
          {
            "id": 2580801244854878,
            "definition": {
              "type": "note",
              "content": "The memory chart draws its threshold marker dynamically based on the available memory on your hosts. If your runtime is deployed across heterogeneous hosts, the threshold will display both the min and the max values.\n\nFor more details on available host metrics, see datadog's [documentation on the OTEL collectors hostmetrics receiver](https://docs.datadoghq.com/opentelemetry/integrations/host_metrics/?tab=host).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 3,
              "y": 8,
              "width": 3,
              "height": 2
            }
          },
          {
            "id": 7978212210408136,
            "definition": {
              "type": "note",
              "content": "These charts draw their threshold marker dynamically based on the limits specified in Docker for memory and CPU. If your runtime is deployed across heterogeneous container configurations for those values, the threshold will display both the min and the max values.\n\nFor more details on available Docker metrics via the Datadog Docker Agent, see the full [documentation on the Datadog Docker Agent](https://docs.datadoghq.com/containers/docker).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 8,
              "width": 3,
              "height": 2
            }
          },
          {
            "id": 548917227211873,
            "definition": {
              "type": "note",
              "content": "These charts draw their threshold marker dynamically based on the limits specified in Docker for memory and CPU. If your runtime is deployed across heterogeneous container configurations for those values, the threshold will display both the min and the max values.\n\nFor more details on available Docker metrics via the OTEL collector, see Datadog's [documentation on the OTEL collector Docker/stats receiver](https://docs.datadoghq.com/opentelemetry/integrations/docker_metrics).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 9,
              "y": 8,
              "width": 3,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 134,
        "width": 12,
        "height": 11
      }
    },
    {
      "id": 7179021198358280,
      "definition": {
        "title": "Coprocessors",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 2752267629213285,
            "definition": {
              "type": "note",
              "content": "If you incorporate [coprocessors](https://www.apollographql.com/docs/graphos/routing/customization/coprocessor) into your runtime, these metrics will be emitted. They indicate basic health and performance impact of the coprocessors on your request lifecycle. In order to correlate any issues we highly recommend you add your own instrumentation to this section to include any health and performance metrics specific to the coprocessors you run and their own request lifecycle.",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 2
            }
          },
          {
            "id": 7202644430123243,
            "definition": {
              "title": "Request Duration",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:apollo.router.operations.coprocessor.duration{service:$service.value,env:$env.value,version:$version.value} by {coprocessor.stage}"
                    }
                  ],
                  "formulas": [
                    {
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ]
            },
            "layout": {
              "x": 0,
              "y": 2,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 8422381032516479,
            "definition": {
              "title": "Request Count",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "sum:apollo.router.operations.coprocessor{service:$service.value,env:$env.value,version:$version.value} by {coprocessor.stage}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "coprocessor request count",
                      "formula": "query0"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "bars"
                }
              ]
            },
            "layout": {
              "x": 6,
              "y": 2,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 7167731681168763,
            "definition": {
              "type": "note",
              "content": "This chart shows the average duration added by coprocessor invocations at each stage of the request lifecycle.\n\n**Why it matters**\n\n-   Coprocessors introduce additional network calls — even fast ones add latency.\n-   High or rising durations can affect overall request performance.\n-   Each stage (e.g., request, supergraph, subgraph) may have different performance profiles depending on configuration and coprocessor logic.\n\n**What to look for**\n\n-   Low, stable durations suggest efficient coprocessor behavior.\n-   Spikes may indicate backend slowness, increased payload size, or external service degradation.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 6,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 8612773097510573,
            "definition": {
              "type": "note",
              "content": "This chart breaks down how often coprocessors are invoked at each stage of the request lifecycle.\n\n**Why it matters**\n\n-   Helps validate where and how often your coprocessors are engaged.\n-   Useful for confirming configuration and understanding the load placed on each external service.\n-   Invocation patterns typically align with overall request volume, but may vary by routing logic or stage-specific conditions.\n\n**What to look for**\n\n-   Consistent counts per stage indicate expected behavior.\n-   Unexpected drops or surges may reflect routing changes, config issues, or stage-specific errors.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 6,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 1752147216961256,
            "definition": {
              "title": "Success Rate",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query0",
                      "query": "sum:apollo.router.operations.coprocessor{service:$service.value,env:$env.value,version:$version.value, coprocessor.succeeded:true} by {coprocessor.stage}.as_count()"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.operations.coprocessor{service:$service.value,env:$env.value,version:$version.value} by {coprocessor.stage}.as_count()"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "coprocessor success rate",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "percent"
                        }
                      },
                      "formula": "anomalies((query0 / query1) * 100, 'basic', 1)"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ],
              "yaxis": {
                "include_zero": true
              }
            },
            "layout": {
              "x": 0,
              "y": 8,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 3453193672865711,
            "definition": {
              "type": "note",
              "content": "This chart tracks the success rate of coprocessor invocations across each stage of the request lifecycle.\n\n**Why it matters**\n\n-   Coprocessors are on the critical path — failures here can short-circuit requests before they reach subgraphs.\n-   A drop in success rate will correlate with elevated error responses at the supergraph level.\n-   Helps isolate external causes of failure when subgraphs appear healthy.\n\n**What to look for**\n\n-   A consistent 100% success rate is expected for stable, production-grade coprocessors.\n-   Any dip may signal availability issues, timeouts, or bugs in coprocessor logic.\n-   If supergraph error rates rise while subgraphs remain unaffected, investigate this metric as a potential root cause.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 12,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 145,
        "width": 12,
        "height": 15
      }
    },
    {
      "id": 4549852525386523,
      "definition": {
        "title": "Sentinel Metrics",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 3582118222557061,
            "definition": {
              "type": "note",
              "content": "This section is a collection of assorted metrics that do not provide directly actionable data, but still may be useful as sentinel values to help diagnose problems occurring in other charts.",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 1
            }
          },
          {
            "id": 6734123939129048,
            "definition": {
              "title": "Uplink and Licensing",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:apollo.router.uplink.fetch.duration.seconds{service:$service.value , env:$env.value, version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "uplink fetch duration avg",
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                },
                {
                  "on_right_yaxis": true,
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "sum:apollo.router.lifecycle.license{service:$service.value , env:$env.value, version:$version.value ,license.state:licensed}"
                    },
                    {
                      "data_source": "metrics",
                      "name": "query2",
                      "query": "sum:apollo.router.lifecycle.license{service:$service.value , env:$env.value, version:$version.value}"
                    }
                  ],
                  "formulas": [
                    {
                      "alias": "licensed routers",
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "percent"
                        }
                      },
                      "formula": "(query1 / query2) * 100"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "line_type": "dashed",
                    "line_width": "thin"
                  },
                  "display_type": "line"
                }
              ]
            },
            "layout": {
              "x": 0,
              "y": 1,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 850320612557659,
            "definition": {
              "title": "Open Connections by Schema and Launch ID",
              "title_size": "16",
              "title_align": "left",
              "show_legend": true,
              "legend_layout": "auto",
              "legend_columns": [
                "avg",
                "min",
                "max",
                "value",
                "sum"
              ],
              "type": "timeseries",
              "requests": [
                {
                  "response_format": "timeseries",
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:apollo.router.open_connections{service:$service.value , env:$env.value , version:$version.value} by {schema.id,launch.id}"
                    }
                  ],
                  "formulas": [
                    {
                      "formula": "query1"
                    }
                  ],
                  "style": {
                    "palette": "dog_classic",
                    "order_by": "values",
                    "line_type": "solid",
                    "line_width": "normal"
                  },
                  "display_type": "line"
                }
              ]
            },
            "layout": {
              "x": 6,
              "y": 1,
              "width": 6,
              "height": 4
            }
          },
          {
            "id": 4964203965396888,
            "definition": {
              "type": "note",
              "content": "[Uplink is used to retrieve your GraphOS license (where applicable) and published schemas from Apollo](https://www.apollographql.com/docs/graphos/routing/uplink). If Uplink has an outage or incident, your license or schema pulls may fail, and this metric can give you a warning when that is the reason.",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 5,
              "width": 6,
              "height": 2
            }
          },
          {
            "id": 7933157762395926,
            "definition": {
              "type": "note",
              "content": "This graph can help you track the rollout of schema changes across your fleet as a supplemental indicator beyond the query planning cache metrics. We also highly recommend adding markers for new deployments based on your CI system's integration with Datadog where appropriate so that you can correlate those with any incidents as well.\n\nTo learn more about launches and what constitutes a new one, see [the documentation](https://www.apollographql.com/docs/graphos/platform/schema-management/delivery/launch).",
              "background_color": "gray",
              "font_size": "12",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": true,
              "tick_pos": "50%",
              "tick_edge": "top",
              "has_padding": true
            },
            "layout": {
              "x": 6,
              "y": 5,
              "width": 6,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 160,
        "width": 12,
        "height": 8
      }
    },
    {
      "id": 3840462392711525,
      "definition": {
        "title": "Resource Estimator",
        "background_color": "gray",
        "show_title": true,
        "type": "group",
        "layout_type": "ordered",
        "widgets": [
          {
            "id": 917215964447150,
            "definition": {
              "type": "note",
              "content": "This section provides all of the metrics necessary to fill out the [resource estimator](https://www.apollographql.com/docs/graphos/routing/self-hosted/resource-estimator) when determining what resources you need to allocate to your runtime deployment in order to handle your traffic patterns. This section is worth revisiting periodically to ensure that your allocations are still correct.",
              "background_color": "white",
              "font_size": "14",
              "text_align": "left",
              "vertical_align": "top",
              "show_tick": false,
              "tick_pos": "50%",
              "tick_edge": "left",
              "has_padding": true
            },
            "layout": {
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 1
            }
          },
          {
            "id": 5911329019154676,
            "definition": {
              "title": "Average Request Rate",
              "title_size": "16",
              "title_align": "left",
              "type": "query_value",
              "requests": [
                {
                  "formulas": [
                    {
                      "number_format": {
                        "unit": {
                          "type": "custom_unit_label",
                          "label": "/s"
                        }
                      },
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:http.server.request.duration{service:$service.value}.as_rate()",
                      "aggregator": "avg"
                    }
                  ],
                  "response_format": "scalar"
                }
              ],
              "autoscale": true,
              "precision": 2,
              "timeseries_background": {
                "type": "area"
              }
            },
            "layout": {
              "x": 0,
              "y": 1,
              "width": 3,
              "height": 2
            }
          },
          {
            "id": 1096965939863061,
            "definition": {
              "title": "Peak Request Rate",
              "title_size": "16",
              "title_align": "left",
              "type": "query_value",
              "requests": [
                {
                  "formulas": [
                    {
                      "number_format": {
                        "unit": {
                          "type": "custom_unit_label",
                          "label": "/s"
                        }
                      },
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "count:http.server.request.duration{service:$service.value}.as_rate()",
                      "aggregator": "max"
                    }
                  ],
                  "response_format": "scalar"
                }
              ],
              "autoscale": true,
              "precision": 2,
              "timeseries_background": {
                "yaxis": {
                  "include_zero": true
                },
                "type": "area"
              }
            },
            "layout": {
              "x": 3,
              "y": 1,
              "width": 3,
              "height": 2
            }
          },
          {
            "id": 3963327310087090,
            "definition": {
              "title": "Baseline Subgraph Latency",
              "title_size": "16",
              "title_align": "left",
              "type": "query_value",
              "requests": [
                {
                  "formulas": [
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "second"
                        },
                        "unit_scale": {
                          "type": "canonical_unit",
                          "unit_name": "millisecond"
                        }
                      },
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:http.client.request.duration{service:$service.value , subgraph.name:*}",
                      "aggregator": "avg"
                    }
                  ],
                  "response_format": "scalar"
                }
              ],
              "autoscale": true,
              "precision": 2,
              "timeseries_background": {
                "type": "area"
              }
            },
            "layout": {
              "x": 6,
              "y": 1,
              "width": 3,
              "height": 2
            }
          },
          {
            "id": 62498923196793,
            "definition": {
              "title": "Average Client Request Size",
              "title_size": "16",
              "title_align": "left",
              "type": "query_value",
              "requests": [
                {
                  "formulas": [
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "byte_in_decimal_bytes_family"
                        },
                        "unit_scale": {
                          "type": "canonical_unit",
                          "unit_name": "megabyte"
                        }
                      },
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:http.client.request.body.size{service:$service.value}",
                      "aggregator": "avg"
                    }
                  ],
                  "response_format": "scalar"
                }
              ],
              "autoscale": true,
              "precision": 2,
              "timeseries_background": {
                "type": "area"
              }
            },
            "layout": {
              "x": 9,
              "y": 1,
              "width": 3,
              "height": 2
            }
          },
          {
            "id": 8060224579298380,
            "definition": {
              "title": "Average Client Response Size",
              "title_size": "16",
              "title_align": "left",
              "type": "query_value",
              "requests": [
                {
                  "formulas": [
                    {
                      "number_format": {
                        "unit": {
                          "type": "canonical_unit",
                          "unit_name": "byte_in_decimal_bytes_family"
                        },
                        "unit_scale": {
                          "type": "canonical_unit",
                          "unit_name": "megabyte"
                        }
                      },
                      "formula": "query1"
                    }
                  ],
                  "queries": [
                    {
                      "data_source": "metrics",
                      "name": "query1",
                      "query": "avg:http.client.response.body.size{service:$service.value}",
                      "aggregator": "avg"
                    }
                  ],
                  "response_format": "scalar"
                }
              ],
              "autoscale": true,
              "precision": 2,
              "timeseries_background": {
                "type": "area"
              }
            },
            "layout": {
              "x": 0,
              "y": 3,
              "width": 3,
              "height": 2
            }
          }
        ]
      },
      "layout": {
        "x": 0,
        "y": 168,
        "width": 12,
        "height": 6
      }
    }
  ],
  "template_variables": [
    {
      "name": "service",
      "prefix": "service",
      "available_values": [],
      "default": "router"
    },
    {
      "name": "env",
      "prefix": "env",
      "available_values": [],
      "default": "prod"
    },
    {
      "name": "version",
      "prefix": "version",
      "available_values": [],
      "default": "*"
    }
  ],
  "layout_type": "ordered",
  "notify_list": [],
  "reflow_type": "fixed"
}
